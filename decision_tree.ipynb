{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris Dataset Info:\n",
      "Total samples: 150\n",
      "Features: sepal_length, sepal_width, petal_length, petal_width\n",
      "Classes: Iris-setosa, Iris-versicolor, Iris-virginica\n",
      "\n",
      "First 5 rows:\n",
      "   sepal_length  sepal_width  petal_length  petal_width        class  target\n",
      "0           5.1          3.5           1.4          0.2  Iris-setosa       0\n",
      "1           4.9          3.0           1.4          0.2  Iris-setosa       0\n",
      "2           4.7          3.2           1.3          0.2  Iris-setosa       0\n",
      "3           4.6          3.1           1.5          0.2  Iris-setosa       0\n",
      "4           5.0          3.6           1.4          0.2  Iris-setosa       0\n",
      "\n",
      "Training samples: 105\n",
      "Testing samples: 45\n",
      "\n",
      "Decision Tree Structure:\n",
      "Node: petal_length <= 1.90, gain=0.3121, samples=105\n",
      "  Left branch:\n",
      "    Leaf: class=Iris-setosa, samples=31\n",
      "  Right branch:\n",
      "    Node: petal_length <= 4.70, gain=0.3551, samples=74\n",
      "      Left branch:\n",
      "        Node: petal_width <= 1.50, gain=0.0588, samples=33\n",
      "          Left branch:\n",
      "            Leaf: class=Iris-versicolor, samples=32\n",
      "          Right branch:\n",
      "            Leaf: class=Iris-virginica, samples=1\n",
      "      Right branch:\n",
      "        Node: petal_width <= 1.70, gain=0.0693, samples=41\n",
      "          Left branch:\n",
      "            Leaf: class=Iris-versicolor, samples=8\n",
      "          Right branch:\n",
      "            Leaf: class=Iris-virginica, samples=33\n",
      "\n",
      "Training Accuracy: 0.9524\n",
      "Testing Accuracy: 0.9556\n",
      "\n",
      "Number of misclassified samples: 2\n",
      "\n",
      "Misclassified samples:\n",
      "Sample 1: True class = Iris-versicolor, Predicted class = Iris-virginica\n",
      "  sepal_length: 6.30\n",
      "  sepal_width: 3.30\n",
      "  petal_length: 4.70\n",
      "  petal_width: 1.60\n",
      "Sample 2: True class = Iris-versicolor, Predicted class = Iris-virginica\n",
      "  sepal_length: 6.00\n",
      "  sepal_width: 3.40\n",
      "  petal_length: 4.50\n",
      "  petal_width: 1.60\n",
      "\n",
      "Feature Importance:\n",
      "sepal_length: 0.0000\n",
      "sepal_width: 0.0000\n",
      "petal_length: 0.9251\n",
      "petal_width: 0.0749\n"
     ]
    }
   ],
   "source": [
    "##Decision Tree Classifier for Iris Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import io\n",
    "\n",
    "class DecisionTreeClassifier:\n",
    "    \"\"\"\n",
    "    A decision tree classifier implementation using only NumPy and pandas.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_depth=None, min_samples_split=2, min_impurity_decrease=0.0):\n",
    "        \"\"\"\n",
    "        Initialize the decision tree classifier.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        max_depth : int or None\n",
    "            Maximum depth of the tree. If None, the tree will expand until all leaves are pure.\n",
    "        min_samples_split : int\n",
    "            Minimum number of samples required to split an internal node.\n",
    "        min_impurity_decrease : float\n",
    "            A node will be split if this split induces a decrease of the impurity greater than this value.\n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_impurity_decrease = min_impurity_decrease\n",
    "        self.tree = None\n",
    "        self.feature_names = None\n",
    "        self.class_names = None\n",
    "    \n",
    "    def _gini(self, y):\n",
    "        \"\"\"\n",
    "        Calculate the Gini impurity of a set of labels.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        y : array-like\n",
    "            Array of labels.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        float\n",
    "            Gini impurity.\n",
    "        \"\"\"\n",
    "        if len(y) == 0:\n",
    "            return 0\n",
    "        \n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        probabilities = counts / len(y)\n",
    "        gini = 1 - np.sum(probabilities ** 2)\n",
    "        return gini\n",
    "    \n",
    "    def _information_gain(self, y, y_left, y_right):\n",
    "        \"\"\"\n",
    "        Calculate the information gain of a split.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        y : array-like\n",
    "            Array of labels before the split.\n",
    "        y_left : array-like\n",
    "            Array of labels in the left branch after the split.\n",
    "        y_right : array-like\n",
    "            Array of labels in the right branch after the split.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        float\n",
    "            Information gain.\n",
    "        \"\"\"\n",
    "        parent_impurity = self._gini(y)\n",
    "        n = len(y)\n",
    "        n_left, n_right = len(y_left), len(y_right)\n",
    "        \n",
    "        if n_left == 0 or n_right == 0:\n",
    "            return 0\n",
    "        \n",
    "        child_impurity = (n_left / n) * self._gini(y_left) + (n_right / n) * self._gini(y_right)\n",
    "        return parent_impurity - child_impurity\n",
    "    \n",
    "    def _best_split(self, X, y):\n",
    "        \"\"\"\n",
    "        Find the best split for a node.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : 2d array-like\n",
    "            The feature data for the node.\n",
    "        y : array-like\n",
    "            The target data for the node.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary containing the best feature index, threshold, left indices, and right indices.\n",
    "        \"\"\"\n",
    "        m, n = X.shape\n",
    "        if m <= self.min_samples_split:\n",
    "            return None\n",
    "        \n",
    "        best_gain = 0.0\n",
    "        best_split = None\n",
    "        \n",
    "        for feature_idx in range(n):\n",
    "            feature_values = X[:, feature_idx]\n",
    "            thresholds = np.unique(feature_values)\n",
    "            \n",
    "            for threshold in thresholds:\n",
    "                left_indices = np.where(feature_values <= threshold)[0]\n",
    "                right_indices = np.where(feature_values > threshold)[0]\n",
    "                \n",
    "                if len(left_indices) == 0 or len(right_indices) == 0:\n",
    "                    continue\n",
    "                \n",
    "                y_left, y_right = y[left_indices], y[right_indices]\n",
    "                gain = self._information_gain(y, y_left, y_right)\n",
    "                \n",
    "                if gain > best_gain and gain > self.min_impurity_decrease:\n",
    "                    best_gain = gain\n",
    "                    best_split = {\n",
    "                        'feature_idx': feature_idx,\n",
    "                        'threshold': threshold,\n",
    "                        'left_indices': left_indices,\n",
    "                        'right_indices': right_indices,\n",
    "                        'gain': gain\n",
    "                    }\n",
    "        \n",
    "        return best_split\n",
    "    \n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        \"\"\"\n",
    "        Recursively build the decision tree.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : 2d array-like\n",
    "            The feature data.\n",
    "        y : array-like\n",
    "            The target data.\n",
    "        depth : int\n",
    "            Current depth of the tree.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            A node of the decision tree.\n",
    "        \"\"\"\n",
    "        # Get the majority class for this node\n",
    "        unique_classes, counts = np.unique(y, return_counts=True)\n",
    "        majority_class = unique_classes[np.argmax(counts)]\n",
    "        \n",
    "        # Create a leaf node if stopping criteria are met\n",
    "        if (self.max_depth is not None and depth >= self.max_depth) or len(np.unique(y)) == 1:\n",
    "            return {'type': 'leaf', 'class': majority_class, 'samples': len(y)}\n",
    "        \n",
    "        # Find the best split\n",
    "        best_split = self._best_split(X, y)\n",
    "        \n",
    "        # If no split improves impurity, create a leaf node\n",
    "        if best_split is None:\n",
    "            return {'type': 'leaf', 'class': majority_class, 'samples': len(y)}\n",
    "        \n",
    "        # Create a decision node\n",
    "        left_indices = best_split['left_indices']\n",
    "        right_indices = best_split['right_indices']\n",
    "        \n",
    "        # Recursively build the left and right branches\n",
    "        left_branch = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n",
    "        right_branch = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n",
    "        \n",
    "        return {\n",
    "            'type': 'node',\n",
    "            'feature_idx': best_split['feature_idx'],\n",
    "            'threshold': best_split['threshold'],\n",
    "            'gain': best_split['gain'],\n",
    "            'samples': len(y),\n",
    "            'left': left_branch,\n",
    "            'right': right_branch\n",
    "        }\n",
    "    \n",
    "    def fit(self, X, y, feature_names=None, class_names=None):\n",
    "        \"\"\"\n",
    "        Build the decision tree.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like or pandas DataFrame of shape (n_samples, n_features)\n",
    "            The training input samples.\n",
    "        y : array-like or pandas Series of shape (n_samples,)\n",
    "            The target values.\n",
    "        feature_names : list, optional\n",
    "            Names of the features. If None and X is a DataFrame, column names are used.\n",
    "        class_names : list, optional\n",
    "            Names of the classes. If None and y is a Series, unique values are used.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        self\n",
    "            Fitted estimator.\n",
    "        \"\"\"\n",
    "        # Store feature names\n",
    "        if feature_names is not None:\n",
    "            self.feature_names = feature_names\n",
    "        elif isinstance(X, pd.DataFrame):\n",
    "            self.feature_names = X.columns.tolist()\n",
    "        else:\n",
    "            self.feature_names = [f\"feature_{i}\" for i in range(X.shape[1])]\n",
    "        \n",
    "        # Store class names\n",
    "        if class_names is not None:\n",
    "            self.class_names = class_names\n",
    "        elif isinstance(y, pd.Series):\n",
    "            self.class_names = y.unique().tolist()\n",
    "        else:\n",
    "            self.class_names = [str(c) for c in np.unique(y)]\n",
    "        \n",
    "        # Convert pandas DataFrame/Series to numpy arrays if necessary\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        if isinstance(y, pd.Series):\n",
    "            y = y.values\n",
    "        \n",
    "        # Convert X to 2D array if it's 1D\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(-1, 1)\n",
    "            \n",
    "        # Build the tree\n",
    "        self.tree = self._build_tree(X, y)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _predict_one(self, x, node):\n",
    "        \"\"\"\n",
    "        Predict the class for a single sample.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : array-like\n",
    "            The input sample.\n",
    "        node : dict\n",
    "            The current node in the tree.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        The predicted class.\n",
    "        \"\"\"\n",
    "        if node['type'] == 'leaf':\n",
    "            return node['class']\n",
    "        \n",
    "        if x[node['feature_idx']] <= node['threshold']:\n",
    "            return self._predict_one(x, node['left'])\n",
    "        else:\n",
    "            return self._predict_one(x, node['right'])\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict classes for samples in X.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like or pandas DataFrame of shape (n_samples, n_features)\n",
    "            The input samples.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        array-like\n",
    "            The predicted classes.\n",
    "        \"\"\"\n",
    "        if self.tree is None:\n",
    "            raise Exception(\"Tree not fitted yet. Call 'fit' first.\")\n",
    "        \n",
    "        # Convert pandas DataFrame to numpy array if necessary\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        \n",
    "        # Convert X to 2D array if it's 1D\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(-1, 1)\n",
    "        \n",
    "        # Predict each sample\n",
    "        return np.array([self._predict_one(x, self.tree) for x in X])\n",
    "    \n",
    "    def print_tree(self, node=None, indent=\"\"):\n",
    "        \"\"\"\n",
    "        Print the tree structure.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        node : dict, optional\n",
    "            The node to start printing from. If None, start from the root.\n",
    "        indent : str, optional\n",
    "            The indentation string for the current level.\n",
    "        \"\"\"\n",
    "        if node is None:\n",
    "            if self.tree is None:\n",
    "                print(\"Tree not fitted yet.\")\n",
    "                return\n",
    "            node = self.tree\n",
    "        \n",
    "        if node['type'] == 'leaf':\n",
    "            class_name = self.class_names[node['class']] if self.class_names is not None else node['class']\n",
    "            print(f\"{indent}Leaf: class={class_name}, samples={node['samples']}\")\n",
    "        else:\n",
    "            feature_name = self.feature_names[node['feature_idx']] if self.feature_names is not None else f\"feature_{node['feature_idx']}\"\n",
    "            print(f\"{indent}Node: {feature_name} <= {node['threshold']:.2f}, gain={node['gain']:.4f}, samples={node['samples']}\")\n",
    "            print(f\"{indent}  Left branch:\")\n",
    "            self.print_tree(node['left'], indent + \"    \")\n",
    "            print(f\"{indent}  Right branch:\")\n",
    "            self.print_tree(node['right'], indent + \"    \")\n",
    "\n",
    "\n",
    "# Function to load the Iris dataset from the UCI repository\n",
    "def load_iris_data():\n",
    "    # URL for the Iris dataset\n",
    "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "    \n",
    "    # Feature names for the Iris dataset\n",
    "    feature_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "    \n",
    "    # Class names for the Iris dataset\n",
    "    class_names = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\n",
    "    \n",
    "    try:\n",
    "        # Download the dataset\n",
    "        response = urllib.request.urlopen(url)\n",
    "        data = response.read().decode('utf-8')\n",
    "        \n",
    "        # Create a pandas DataFrame\n",
    "        df = pd.read_csv(io.StringIO(data), header=None, names=feature_names + ['class'])\n",
    "        \n",
    "        # Convert class names to numeric targets\n",
    "        class_to_idx = {name: idx for idx, name in enumerate(class_names)}\n",
    "        df['target'] = df['class'].map(class_to_idx)\n",
    "        \n",
    "        return df, feature_names, class_names\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Iris dataset: {e}\")\n",
    "        \n",
    "        # Fallback: Create a simple dataset with the Iris structure\n",
    "        print(\"Using hardcoded Iris dataset instead...\")\n",
    "        \n",
    "        # Define a small sample of the Iris dataset\n",
    "        data = [\n",
    "            # Setosa examples\n",
    "            [5.1, 3.5, 1.4, 0.2, 'Iris-setosa', 0],\n",
    "            [4.9, 3.0, 1.4, 0.2, 'Iris-setosa', 0],\n",
    "            [4.7, 3.2, 1.3, 0.2, 'Iris-setosa', 0],\n",
    "            [4.6, 3.1, 1.5, 0.2, 'Iris-setosa', 0],\n",
    "            [5.0, 3.6, 1.4, 0.2, 'Iris-setosa', 0],\n",
    "            # Versicolor examples\n",
    "            [7.0, 3.2, 4.7, 1.4, 'Iris-versicolor', 1],\n",
    "            [6.4, 3.2, 4.5, 1.5, 'Iris-versicolor', 1],\n",
    "            [6.9, 3.1, 4.9, 1.5, 'Iris-versicolor', 1],\n",
    "            [5.5, 2.3, 4.0, 1.3, 'Iris-versicolor', 1],\n",
    "            [6.5, 2.8, 4.6, 1.5, 'Iris-versicolor', 1],\n",
    "            # Virginica examples\n",
    "            [6.3, 3.3, 6.0, 2.5, 'Iris-virginica', 2],\n",
    "            [5.8, 2.7, 5.1, 1.9, 'Iris-virginica', 2],\n",
    "            [7.1, 3.0, 5.9, 2.1, 'Iris-virginica', 2],\n",
    "            [6.3, 2.9, 5.6, 1.8, 'Iris-virginica', 2],\n",
    "            [6.5, 3.0, 5.8, 2.2, 'Iris-virginica', 2]\n",
    "        ]\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(data, columns=feature_names + ['class', 'target'])\n",
    "        \n",
    "        return df, feature_names, class_names\n",
    "\n",
    "\n",
    "# Function to split the data into training and testing sets\n",
    "def train_test_split(X, y, test_size=0.3, random_state=None):\n",
    "    \"\"\"\n",
    "    Split the data into training and testing sets.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like or pandas DataFrame\n",
    "        Features.\n",
    "    y : array-like or pandas Series\n",
    "        Target.\n",
    "    test_size : float, default=0.3\n",
    "        Proportion of the dataset to include in the test split.\n",
    "    random_state : int, default=None\n",
    "        Controls the shuffling applied to the data.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    X_train, X_test, y_train, y_test\n",
    "        Splits of X and y.\n",
    "    \"\"\"\n",
    "    # Set random seed if specified\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    # Get the number of samples\n",
    "    n_samples = len(X)\n",
    "    \n",
    "    # Get the number of test samples\n",
    "    n_test = int(n_samples * test_size)\n",
    "    \n",
    "    # Generate random indices for the test set\n",
    "    indices = np.random.permutation(n_samples)\n",
    "    test_indices = indices[:n_test]\n",
    "    train_indices = indices[n_test:]\n",
    "    \n",
    "    # Split X\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X_train = X.iloc[train_indices]\n",
    "        X_test = X.iloc[test_indices]\n",
    "    else:\n",
    "        X_train = X[train_indices]\n",
    "        X_test = X[test_indices]\n",
    "    \n",
    "    # Split y\n",
    "    if isinstance(y, pd.Series):\n",
    "        y_train = y.iloc[train_indices]\n",
    "        y_test = y.iloc[test_indices]\n",
    "    else:\n",
    "        y_train = y[train_indices]\n",
    "        y_test = y[test_indices]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "# Main function to demonstrate the decision tree on the Iris dataset\n",
    "def main():\n",
    "    # Load the Iris dataset\n",
    "    df, feature_names, class_names = load_iris_data()\n",
    "    \n",
    "    # Print dataset info\n",
    "    print(\"Iris Dataset Info:\")\n",
    "    print(f\"Total samples: {len(df)}\")\n",
    "    print(f\"Features: {', '.join(feature_names)}\")\n",
    "    print(f\"Classes: {', '.join(class_names)}\")\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # Split the data into features and target\n",
    "    X = df.drop(['class', 'target'], axis=1)\n",
    "    y = df['target']\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTraining samples: {len(X_train)}\")\n",
    "    print(f\"Testing samples: {len(X_test)}\")\n",
    "    \n",
    "    # Create and train the decision tree\n",
    "    tree = DecisionTreeClassifier(max_depth=3)\n",
    "    tree.fit(X_train, y_train, feature_names=feature_names, class_names=class_names)\n",
    "    \n",
    "    # Print the tree structure\n",
    "    print(\"\\nDecision Tree Structure:\")\n",
    "    tree.print_tree()\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_train = tree.predict(X_train)\n",
    "    y_pred_test = tree.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    train_accuracy = np.mean(y_pred_train == y_train.values)\n",
    "    test_accuracy = np.mean(y_pred_test == y_test.values)\n",
    "    \n",
    "    print(f\"\\nTraining Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    # Analyze misclassifications\n",
    "    if isinstance(y_test, pd.Series):\n",
    "        misclassified_indices = np.where(y_pred_test != y_test.values)[0]\n",
    "        misclassified = X_test.iloc[misclassified_indices]\n",
    "        misclassified_true = y_test.iloc[misclassified_indices]\n",
    "        misclassified_pred = y_pred_test[misclassified_indices]\n",
    "    else:\n",
    "        misclassified_indices = np.where(y_pred_test != y_test)[0]\n",
    "        misclassified = X_test[misclassified_indices]\n",
    "        misclassified_true = y_test[misclassified_indices]\n",
    "        misclassified_pred = y_pred_test[misclassified_indices]\n",
    "    \n",
    "    if len(misclassified) > 0:\n",
    "        print(f\"\\nNumber of misclassified samples: {len(misclassified)}\")\n",
    "        print(\"\\nMisclassified samples:\")\n",
    "        for i in range(len(misclassified)):\n",
    "            true_class_idx = misclassified_true.iloc[i] if isinstance(misclassified_true, pd.Series) else misclassified_true[i]\n",
    "            pred_class_idx = misclassified_pred[i]\n",
    "            true_class = class_names[true_class_idx]\n",
    "            pred_class = class_names[pred_class_idx]\n",
    "            print(f\"Sample {i+1}: True class = {true_class}, Predicted class = {pred_class}\")\n",
    "            \n",
    "            if isinstance(misclassified, pd.DataFrame):\n",
    "                for feature in feature_names:\n",
    "                    print(f\"  {feature}: {misclassified[feature].iloc[i]:.2f}\")\n",
    "            else:\n",
    "                for j, feature in enumerate(feature_names):\n",
    "                    print(f\"  {feature}: {misclassified[i, j]:.2f}\")\n",
    "    else:\n",
    "        print(\"\\nNo misclassified samples in the test set!\")\n",
    "    \n",
    "    # Feature importance analysis\n",
    "    def get_feature_importance(node, importance_scores):\n",
    "        \"\"\"\n",
    "        Extract feature importance from the tree structure.\n",
    "        \"\"\"\n",
    "        if node['type'] == 'node':\n",
    "            # Add this node's information gain to the feature's importance\n",
    "            importance_scores[node['feature_idx']] += node['gain'] * node['samples']\n",
    "            \n",
    "            # Recursively process child nodes\n",
    "            get_feature_importance(node['left'], importance_scores)\n",
    "            get_feature_importance(node['right'], importance_scores)\n",
    "    \n",
    "    # Initialize feature importance scores\n",
    "    importance_scores = np.zeros(len(feature_names))\n",
    "    \n",
    "    # Calculate feature importance\n",
    "    get_feature_importance(tree.tree, importance_scores)\n",
    "    \n",
    "    # Normalize importance scores\n",
    "    if np.sum(importance_scores) > 0:\n",
    "        importance_scores = importance_scores / np.sum(importance_scores)\n",
    "    \n",
    "    # Print feature importance\n",
    "    print(\"\\nFeature Importance:\")\n",
    "    for i, (feature, importance) in enumerate(zip(feature_names, importance_scores)):\n",
    "        print(f\"{feature}: {importance:.4f}\")\n",
    "    \n",
    "    # Return the trained model for further analysis if needed\n",
    "    return tree, X_test, y_test\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
